{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1ME8m2EE3c4"
      },
      "source": [
        "# Agentic RAG in LlamaIndex\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 0: Loading libraries"
      ],
      "metadata": {
        "id": "-M4zgrIYtzEW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gpk5DaC8E3c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "045a6905-20d7-4f94-bcf9-2eec753c71ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m20.8/20.8 MB\u001b[0m \u001b[31m94.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m108.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m303.3/303.3 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m84.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m88.5/88.5 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m63.9/63.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m98.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m65.9/65.9 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m323.9/323.9 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m88.0/88.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m101.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m144.4/144.4 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "google-adk 1.17.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.38.0 which is incompatible.\n",
            "google-adk 1.17.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install llama-index llama-index-vector-stores-chroma llama-index-llms-huggingface-api llama-index-embeddings-huggingface -U -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\""
      ],
      "metadata": {
        "id": "vobdEXgquH-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QysSnlVpE3c-"
      },
      "source": [
        "And, let's log in to Hugging Face to use serverless Inference APIs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "d309591be2c74ec89237d7dc8cc2d35b",
            "0de60d5681bf4ef9899bf3d25bb0cc7b",
            "1d953736b2b7480d93cd4ba5ee083612",
            "02cb4e3fddbf47aaa90438c34a3aa0ba",
            "f6d6c42ff8aa4267aae810403c3bc472",
            "d916a7725a754b0c97195dda0f81eb0b",
            "8de4f42760c84893ba42a6c21805aaed",
            "41e4aad7ab7849c482b76b63e05c5103",
            "90b457ea50244a4ebe9539792e8d144f",
            "5106674d1e8445418d5ac8b8a47236da",
            "a01f8a252e944911b7f48a9bd7c38d77",
            "83d0691dc83e4b9683bbc0d75681e1cf",
            "7667a19e19894549b60ed2f2444fa1e9",
            "535500bd04794bd085ac26f7d3ca8641",
            "604e4d2cccf54525923376a4d2c9030d",
            "c91f271675064f2199990457b8bee6d2",
            "87ea897cd11045d2a6e9499e1f376cba",
            "2d67efd367d740c58603a4c2cff97043",
            "83cce3969a774ce5b957bc1e8b9067c3",
            "1408f43fc61b45f4978287227ae228d0"
          ]
        },
        "id": "POMls8o8E3c-",
        "outputId": "f835ab83-50d6-4f60-d0e6-7ece35b1660d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv\u2026"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d309591be2c74ec89237d7dc8cc2d35b"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Simple RAG Systems"
      ],
      "metadata": {
        "id": "Wz3QdNYCtU7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, Settings\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "\n",
        "# Load document\n",
        "reader = SimpleDirectoryReader(input_files=[\"state.pdf\"])\n",
        "documents = reader.load_data()\n",
        "print(f\"Loaded {len(documents)} document(s).\")\n",
        "\n",
        "# Split into chunks\n",
        "splitter = SentenceSplitter(chunk_size=1024)\n",
        "nodes = splitter.get_nodes_from_documents(documents)\n",
        "\n",
        "# Set up LLM and embedding model\n",
        "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
        "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\")\n",
        "\n",
        "# Create vector index\n",
        "vector_index = VectorStoreIndex(nodes)\n",
        "\n",
        "# Create query engine\n",
        "query_engine = vector_index.as_query_engine()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUJReWButXJp",
        "outputId": "6b6642ac-6eb0-4d95-b75a-483852b54f5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 26 document(s).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.1 Inspecting the vector store"
      ],
      "metadata": {
        "id": "hNDjd6WKAiGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Access the vector store data directly\n",
        "vector_store = vector_index.vector_store\n",
        "\n",
        "# Get embedding dictionary and node dictionary\n",
        "embedding_dict = vector_store.data.embedding_dict\n",
        "node_dict = vector_store.data.text_id_to_ref_doc_id\n",
        "\n",
        "print(f\"Number of embeddings: {len(embedding_dict)}\")\n",
        "print(f\"Number of node references: {len(node_dict)}\")\n",
        "\n",
        "# Show first few embeddings\n",
        "for i, (node_id, embedding) in enumerate(list(embedding_dict.items())[:3]):\n",
        "    print(f\"\\n--- Embedding {i} ---\")\n",
        "    print(f\"Node ID: {node_id}\")\n",
        "    print(f\"Embedding dimension: {len(embedding)}\")\n",
        "    print(f\"First 10 values: {embedding[:10]}\")"
      ],
      "metadata": {
        "id": "iipL_cCRAhAf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5593d075-db9f-44a0-b0b7-08575976ebe8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of embeddings: 27\n",
            "Number of node references: 27\n",
            "\n",
            "--- Embedding 0 ---\n",
            "Node ID: a997ad8c-500d-4827-b2b4-d5d068a60b0f\n",
            "Embedding dimension: 1536\n",
            "First 10 values: [-0.014133124612271786, -0.01401845458894968, -0.013387767598032951, -0.021386027336120605, 0.006475293077528477, 0.013058089651167393, -0.02142902836203575, 0.01010532770305872, -0.03012964315712452, -0.024381790310144424]\n",
            "\n",
            "--- Embedding 1 ---\n",
            "Node ID: 945aeda2-1ab4-4d5f-9a23-3bcaeae2f6ca\n",
            "Embedding dimension: 1536\n",
            "First 10 values: [-0.014858749695122242, -0.02170269563794136, -0.003049110062420368, -0.019040387123823166, -0.007666334044188261, 0.02011367306113243, -0.028435129672288895, 0.010872256010770798, -0.02122877538204193, -0.02844906970858574]\n",
            "\n",
            "--- Embedding 2 ---\n",
            "Node ID: 5b92bddc-68d8-4c70-9c07-2784b57de187\n",
            "Embedding dimension: 1536\n",
            "First 10 values: [0.0016929084667935967, -0.027508273720741272, -0.004747966304421425, -0.03599747642874718, -0.007251191884279251, 0.01682875119149685, -0.01629817672073841, 0.02659677341580391, -0.040976718068122864, -0.020107433199882507]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.2 Asking questions to the RAG system"
      ],
      "metadata": {
        "id": "6caODl2LAkuf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Query the document\n",
        "response = query_engine.query(\"Who is Lareina Yee?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQwyzi6StoOE",
        "outputId": "9b7f31d0-4017-47dc-b834-49c4f084fcfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lareina Yee is a Senior partner and McKinsey Global Institute director.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.3 Checking if the responses make sense"
      ],
      "metadata": {
        "id": "4FrcVI5_AoKj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(response.source_nodes))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzWW3i6d--cv",
        "outputId": "492d209e-bbdc-4fba-8aab-d42c4535d8ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print out each source node\n",
        "print(\"Source nodes:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for i, node in enumerate(response.source_nodes):\n",
        "    print(f\"Node {i+1}:\")\n",
        "    print(f\"Score: {node.score}\")\n",
        "    print(f\"Text: {node.text}\")\n",
        "    print(f\"Metadata: {node.metadata}\")\n",
        "    print(\"-\" * 30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSUGVGT1ACSL",
        "outputId": "66dfbb46-fbde-4a93-dd2a-8ceb4fbd6cf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source nodes:\n",
            "==================================================\n",
            "Node 1:\n",
            "Score: 0.7230227134327099\n",
            "Text: The state of AI  \n",
            "March 2025\n",
            "Alex Singla  \n",
            "Alexander Sukharevsky  \n",
            "Lareina Yee  \n",
            "Michael Chui  \n",
            "Bryce Hall\n",
            "How organizations are rewiring to capture value\n",
            "Metadata: {'page_label': '1', 'file_name': 'state.pdf', 'file_path': 'state.pdf', 'file_type': 'application/pdf', 'file_size': 5564174, 'creation_date': '2025-07-01', 'last_modified_date': '2025-07-01'}\n",
            "------------------------------\n",
            "Node 2:\n",
            "Score: 0.7062869479601049\n",
            "Text: McKinsey commentary\n",
            "Lareina Yee\n",
            "Senior partner and McKinsey Global Institute director\n",
            "Although we remain in the early stages of gen AI, we\u2019re beginning to get a glimpse into \n",
            "the ways the technology is affecting the workforce. A common fear about the technology \n",
            "is that it will be a job killer, as organizations offload tasks historically done by employees to \n",
            "increasingly powerful AI platforms. But our survey suggests that this is not necessarily the \n",
            "case. In fact, a plurality of respondents anticipate no immediate change to the size of their \n",
            "workforces. And while respondents expect lower head counts in some functions\u2014such as \n",
            "service operations and supply chain/inventory management\u2014in other functions\u2014including \n",
            "software engineering and product development\u2014respondents are actually anticipating an \n",
            "increase in the number of employees. \n",
            "Meantime, the difficulty of finding AI talent, while still considerable, is beginning to ease. \n",
            "Perhaps more people are taking the initiative to enhance their own capabilities. Or it could be \n",
            "that corporate investments in upskilling are beginning to bear fruit. Both of these somewhat \n",
            "counterintuitive trends serve to reinforce the fact that we are still in the early days of the AI \n",
            "revolution\u2014the long-term workforce effects are still only beginning to take shape.   \n",
            "AI use continues to climb\n",
            " \n",
            "Reported use of AI increased in 2024.3 In the latest survey, 78 percent of respondents say \n",
            "their organizations use AI in at least one business function, up from 72 percent in early 2024 \n",
            "and 55 percent a year earlier (Exhibit 8). Respondents most often report using the technology \n",
            "in the IT and marketing and sales functions, followed by service operations. The business \n",
            "function that saw the largest increase in AI use in the past six months is IT, where the share of \n",
            "respondents reporting AI use jumped from 27 percent to 36 percent. \n",
            "Organizations are also using AI in more business functions than in the previous State of AI \n",
            "survey. For the first time, most survey respondents report the use of AI in more than one \n",
            "business function (Exhibit 9). Responses show organizations using AI in an average of three \n",
            "business functions\u2014an increase from early 2024, but still a minority of functions.\n",
            "3 The survey question asked, \u201cIn which business functions has your organization adopted AI (for example, machine learning, \n",
            "computer vision, natural-language processing)?\u201d Eleven business functions were offered as answer choices. Organizations \n",
            "using AI are those that, according to respondents, have adopted AI in at least one business function. For the purposes of our \n",
            "research, we left \u201cadopted\u201d undefined. Use of AI, therefore, spans from early experimentation by a few employees to AI being \n",
            "embedded across multiple business units that have entirely redesigned their business processes.\n",
            "14The state of AI: How organizations are rewiring to capture value\n",
            "Metadata: {'page_label': '15', 'file_name': 'state.pdf', 'file_path': 'state.pdf', 'file_type': 'application/pdf', 'file_size': 5564174, 'creation_date': '2025-07-01', 'last_modified_date': '2025-07-01'}\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ask more questions\n",
        "response2 = query_engine.query(\"What are the main findings about AI adoption?\")\n",
        "print(response2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I59dl1cstpOY",
        "outputId": "7825d4bc-ea61-49f6-e6fe-edd7a341dc11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The main findings about AI adoption include an increase in reported AI use among organizations, with 78 percent of respondents stating their organizations use AI in at least one business function. The IT and marketing and sales functions are the most common areas where AI is utilized. Additionally, organizations are now using AI in more business functions compared to previous surveys, with most respondents reporting AI use in more than one business function on average.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response3 = query_engine.query(\"What does the document say about AI risks?\")\n",
        "print(response3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pu_T0jLStqU-",
        "outputId": "1d38f476-9922-48b3-cdc6-3134a482be2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The document mentions that organizations are actively working to mitigate risks related to inaccuracy, cybersecurity, intellectual property infringement, and privacy when using generative AI. It also notes that larger organizations are reported to be addressing more risks compared to other organizations, particularly focusing on managing potential cybersecurity and privacy risks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1T80meUE3dA"
      },
      "source": [
        "## Part 2: Agentic RAG\n",
        "\n",
        "Let's now upgrade the previously defined RAG system into an Agentic RAG system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fsJGdIJ3afAH",
        "outputId": "269bee18-f692-41bb-c6f9-8f1393d8fd60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.33.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.13)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.18.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (0.33.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (4.12.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (1.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub) (2025.1.31)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade datasets\n",
        "!pip install --upgrade huggingface-hub"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.1: Loading the data"
      ],
      "metadata": {
        "id": "cAHJ_2iLt-gH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ju5oAHyNX7du",
        "outputId": "20af0139-6619-4d62-9ad5-2eeaecddc525"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 26 document(s).\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "\n",
        "reader = SimpleDirectoryReader(input_files=[\"state.pdf\"])\n",
        "documents = reader.load_data()\n",
        "\n",
        "print(f\"Loaded {len(documents)} document(s).\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.2: Breaking the data into chunks"
      ],
      "metadata": {
        "id": "9HWY21vPuBdw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "\n",
        "# chunk_size of 1024 is a good default value\n",
        "splitter = SentenceSplitter(chunk_size=1024)\n",
        "# Create nodes from documents\n",
        "nodes = splitter.get_nodes_from_documents(documents)"
      ],
      "metadata": {
        "id": "maOPpR4iis3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.3 Define the LLM and the Embedding Model"
      ],
      "metadata": {
        "id": "CJnGnuAZuKzD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import Settings\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "\n",
        "# LLM model\n",
        "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
        "# embedding model\n",
        "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\")"
      ],
      "metadata": {
        "id": "3DCnVNCFizXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.4 Create the vector index and summary index"
      ],
      "metadata": {
        "id": "RZkCDa6UuOgi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SummaryIndex, VectorStoreIndex\n",
        "\n",
        "# summary index\n",
        "summary_index = SummaryIndex(nodes)\n",
        "# vector store index\n",
        "vector_index = VectorStoreIndex(nodes)"
      ],
      "metadata": {
        "id": "aBu3TAWni2Vy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.4 Create the vector query engine and summary query engine"
      ],
      "metadata": {
        "id": "YOUKB4heuSO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# summary query engine\n",
        "summary_query_engine = summary_index.as_query_engine(\n",
        "    response_mode=\"tree_summarize\",\n",
        "    use_async=True,\n",
        ")\n",
        "\n",
        "# vector query engine\n",
        "vector_query_engine = vector_index.as_query_engine()"
      ],
      "metadata": {
        "id": "9l9lVrgNi6Dd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.5 Convert the vector and query engines into tools"
      ],
      "metadata": {
        "id": "YEpulllzuWHI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.tools import QueryEngineTool\n",
        "\n",
        "\n",
        "summary_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=summary_query_engine,\n",
        "    description=(\n",
        "        \"Useful for summarization questions related to the State of AI paper.\"\n",
        "    ),\n",
        ")\n",
        "\n",
        "vector_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=vector_query_engine,\n",
        "    description=(\n",
        "        \"Useful for retrieving specific context from the the State of AI paper.\"\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "UHugM5x-i8Bf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.6 Define a superset query engine"
      ],
      "metadata": {
        "id": "ZYhHCqoLucC7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.query_engine.router_query_engine import RouterQueryEngine\n",
        "from llama_index.core.selectors import LLMSingleSelector\n",
        "\n",
        "\n",
        "query_engine = RouterQueryEngine(\n",
        "    selector=LLMSingleSelector.from_defaults(),\n",
        "    query_engine_tools=[\n",
        "        summary_tool,\n",
        "        vector_tool,\n",
        "    ],\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "LF3YTqDli-FR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.7 Test whether the query engine works"
      ],
      "metadata": {
        "id": "PKbSL_tbuflQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\"Who is Lareina Yee according to teh document?\")\n",
        "print(str(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAA2JIhai_n_",
        "outputId": "446b5976-b6d3-42a0-d40a-10e80c30c241"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;3;38;5;200mSelecting query engine 1: This choice is more relevant as it focuses on retrieving specific context from the document, which would be necessary to identify who Lareina Yee is..\n",
            "\u001b[0mLareina Yee is one of the authors listed in the document.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.8 Convert the query engine into a tool"
      ],
      "metadata": {
        "id": "vjBMU8pwul8P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create tool wrapper around router\n",
        "query_engine_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=query_engine,\n",
        "    name=\"state_of_ai_report_assistant\",\n",
        "    description=\"Answers questions based on the McKinsey 2025 State of AI report.\",\n",
        "    return_direct=False,\n",
        ")\n"
      ],
      "metadata": {
        "id": "-xAHwfwqulcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.9 Define system prompt\n"
      ],
      "metadata": {
        "id": "lcy5m7bYuokd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"\"\"\n",
        "You are a helpful assistant specialized in answering questions using the 'State of AI' March 2025 report by McKinsey.\n",
        "Your task is to:\n",
        "\n",
        "1. Use the Summary Tool when the user asks for high-level insights, trends, survey findings, or general understanding\n",
        "   (e.g., \"What are the top AI adoption practices?\" or \"Summarize the report's key findings\").\n",
        "\n",
        "2. Use the Vector Tool when the user is asking for specific statistics, organizational practices, exhibit-based\n",
        "   evidence, or detailed examples\n",
        "   (e.g., \"What percentage of companies track AI KPIs?\" or \"What are the risks companies are mitigating?\").\n",
        "\n",
        "Refer only to the content of the report. If the user's query is outside this context, politely decline or redirect.\n",
        "\n",
        "Check your answer multiple times to make sure it is actually relevant and mentioned in the document.\n",
        "\n",
        "Examples of summary queries:\n",
        "- \"How are companies restructuring to adopt GenAI?\"\n",
        "- \"What does the report say about workforce reskilling?\"\n",
        "\n",
        "Examples of specific/vector queries:\n",
        "- \"What percentage of companies have a roadmap for GenAI adoption?\"\n",
        "- \"Who is responsible for AI governance in large firms?\"\n",
        "\n",
        "Always explain clearly, referencing exact statistics, frameworks, or concepts when relevant. Be concise and insightful.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "adx0Y8UgmJHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.10 Define the agent"
      ],
      "metadata": {
        "id": "b7xhEtfvut_s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.agent.workflow import AgentWorkflow\n",
        "\n",
        "query_engine_agent = AgentWorkflow.from_tools_or_functions(\n",
        "    tools_or_functions=[query_engine_tool],\n",
        "    llm=Settings.llm,\n",
        "    system_prompt=system_prompt,\n",
        ")\n"
      ],
      "metadata": {
        "id": "bt-1Pnr7jMfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFUqiH8kE3dB"
      },
      "source": [
        "#### 2.11 Setup agent observability using Arize Phoenix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CRE0SnzJeY47",
        "outputId": "a8a23cad-00df-4b4c-b021-66e3f5d63fcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-index-callbacks-arize-phoenix\n",
            "  Downloading llama_index_callbacks_arize_phoenix-0.5.1-py3-none-any.whl.metadata (744 bytes)\n",
            "Collecting arize-phoenix\n",
            "  Downloading arize_phoenix-11.2.0-py3-none-any.whl.metadata (27 kB)\n",
            "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-callbacks-arize-phoenix) (0.12.45)\n",
            "Collecting openinference-instrumentation-llama-index>=4.1.0 (from llama-index-callbacks-arize-phoenix)\n",
            "  Downloading openinference_instrumentation_llama_index-4.3.1-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting aioitertools (from arize-phoenix)\n",
            "  Downloading aioitertools-0.12.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: aiosqlite in /usr/local/lib/python3.11/dist-packages (from arize-phoenix) (0.21.0)\n",
            "Collecting alembic<2,>=1.3.0 (from arize-phoenix)\n",
            "  Downloading alembic-1.16.2-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting arize-phoenix-client (from arize-phoenix)\n",
            "  Downloading arize_phoenix_client-1.11.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting arize-phoenix-evals>=0.20.6 (from arize-phoenix)\n",
            "  Downloading arize_phoenix_evals-0.21.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Collecting arize-phoenix-otel>=0.10.3 (from arize-phoenix)\n",
            "  Downloading arize_phoenix_otel-0.12.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting authlib (from arize-phoenix)\n",
            "  Downloading authlib-1.6.0-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.11/dist-packages (from arize-phoenix) (5.5.2)\n",
            "Collecting email-validator (from arize-phoenix)\n",
            "  Downloading email_validator-2.2.0-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.11/dist-packages (from arize-phoenix) (0.115.13)\n",
            "Requirement already satisfied: grpc-interceptor in /usr/local/lib/python3.11/dist-packages (from arize-phoenix) (0.15.4)\n",
            "Requirement already satisfied: grpcio in /usr/local/lib/python3.11/dist-packages (from arize-phoenix) (1.73.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from arize-phoenix) (0.28.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from arize-phoenix) (3.1.6)\n",
            "Requirement already satisfied: numpy!=2.0.0 in /usr/local/lib/python3.11/dist-packages (from arize-phoenix) (2.0.2)\n",
            "Collecting openinference-instrumentation>=0.1.32 (from arize-phoenix)\n",
            "  Downloading openinference_instrumentation-0.1.34-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting openinference-semantic-conventions>=0.1.20 (from arize-phoenix)\n",
            "  Downloading openinference_semantic_conventions-0.1.21-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting opentelemetry-exporter-otlp (from arize-phoenix)\n",
            "  Downloading opentelemetry_exporter_otlp-1.34.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: opentelemetry-proto>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from arize-phoenix) (1.34.1)\n",
            "Requirement already satisfied: opentelemetry-sdk in /usr/local/lib/python3.11/dist-packages (from arize-phoenix) (1.34.1)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions in /usr/local/lib/python3.11/dist-packages (from arize-phoenix) (0.55b1)\n",
            "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.11/dist-packages (from arize-phoenix) (2.2.2)\n",
            "Requirement already satisfied: protobuf<6.0,>=4.25.8 in /usr/local/lib/python3.11/dist-packages (from arize-phoenix) (5.29.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from arize-phoenix) (5.9.5)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (from arize-phoenix) (18.1.0)\n",
            "Requirement already satisfied: pydantic>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from arize-phoenix) (2.11.7)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.11/dist-packages (from arize-phoenix) (0.0.20)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from arize-phoenix) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from arize-phoenix) (1.15.3)\n",
            "Requirement already satisfied: sqlalchemy<3,>=2.0.4 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy[asyncio]<3,>=2.0.4->arize-phoenix) (2.0.41)\n",
            "Collecting sqlean-py>=3.45.1 (from arize-phoenix)\n",
            "  Downloading sqlean_py-3.49.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: starlette in /usr/local/lib/python3.11/dist-packages (from arize-phoenix) (0.46.2)\n",
            "Collecting strawberry-graphql==0.270.1 (from arize-phoenix)\n",
            "  Downloading strawberry_graphql-0.270.1-py3-none-any.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from arize-phoenix) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6 in /usr/local/lib/python3.11/dist-packages (from arize-phoenix) (4.14.0)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.11/dist-packages (from arize-phoenix) (0.34.3)\n",
            "Requirement already satisfied: wrapt>=1.17.2 in /usr/local/lib/python3.11/dist-packages (from arize-phoenix) (1.17.2)\n",
            "Collecting graphql-core<3.4.0,>=3.2.0 (from strawberry-graphql==0.270.1->arize-phoenix)\n",
            "  Downloading graphql_core-3.2.6-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: packaging>=23 in /usr/local/lib/python3.11/dist-packages (from strawberry-graphql==0.270.1->arize-phoenix) (24.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from strawberry-graphql==0.270.1->arize-phoenix) (2.9.0.post0)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic<2,>=1.3.0->arize-phoenix) (1.1.3)\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-callbacks-arize-phoenix) (3.11.15)\n",
            "Requirement already satisfied: banks<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-callbacks-arize-phoenix) (2.1.3)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-callbacks-arize-phoenix) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-callbacks-arize-phoenix) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2,>=1.0.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-callbacks-arize-phoenix) (1.0.8)\n",
            "Requirement already satisfied: filetype<2,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-callbacks-arize-phoenix) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-callbacks-arize-phoenix) (2025.3.2)\n",
            "Requirement already satisfied: llama-index-workflows<2,>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-callbacks-arize-phoenix) (1.0.1)\n",
            "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-callbacks-arize-phoenix) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-callbacks-arize-phoenix) (3.5)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-callbacks-arize-phoenix) (3.9.1)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-callbacks-arize-phoenix) (11.2.1)\n",
            "Requirement already satisfied: pyyaml>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-callbacks-arize-phoenix) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-callbacks-arize-phoenix) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=80.9.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-callbacks-arize-phoenix) (80.9.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-callbacks-arize-phoenix) (8.5.0)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-callbacks-arize-phoenix) (0.9.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-callbacks-arize-phoenix) (0.9.0)\n",
            "Requirement already satisfied: opentelemetry-api in /usr/local/lib/python3.11/dist-packages (from openinference-instrumentation>=0.1.32->arize-phoenix) (1.34.1)\n",
            "Collecting opentelemetry-instrumentation (from openinference-instrumentation-llama-index>=4.1.0->llama-index-callbacks-arize-phoenix)\n",
            "  Downloading opentelemetry_instrumentation-0.55b1-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->arize-phoenix) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->arize-phoenix) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.1.0->arize-phoenix) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.1.0->arize-phoenix) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.1.0->arize-phoenix) (0.4.1)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy<3,>=2.0.4->sqlalchemy[asyncio]<3,>=2.0.4->arize-phoenix) (3.2.3)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.11/dist-packages (from authlib->arize-phoenix) (43.0.3)\n",
            "Collecting dnspython>=2.0.0 (from email-validator->arize-phoenix)\n",
            "  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: idna>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from email-validator->arize-phoenix) (3.10)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette->arize-phoenix) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx->arize-phoenix) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->arize-phoenix) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->arize-phoenix) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->arize-phoenix) (3.0.2)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc==1.34.1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp->arize-phoenix) (1.34.1)\n",
            "Collecting opentelemetry-exporter-otlp-proto-http==1.34.1 (from opentelemetry-exporter-otlp->arize-phoenix)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_http-1.34.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.34.1->opentelemetry-exporter-otlp->arize-phoenix) (1.70.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.34.1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.34.1->opentelemetry-exporter-otlp->arize-phoenix) (1.34.1)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api->openinference-instrumentation>=0.1.32->arize-phoenix) (8.7.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->arize-phoenix) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->arize-phoenix) (3.6.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn->arize-phoenix) (8.2.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-callbacks-arize-phoenix) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-callbacks-arize-phoenix) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-callbacks-arize-phoenix) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-callbacks-arize-phoenix) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-callbacks-arize-phoenix) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-callbacks-arize-phoenix) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-callbacks-arize-phoenix) (1.20.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette->arize-phoenix) (1.3.1)\n",
            "Requirement already satisfied: griffe in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-callbacks-arize-phoenix) (1.7.3)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-callbacks-arize-phoenix) (4.3.8)\n",
            "Requirement already satisfied: llama-index-instrumentation>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-workflows<2,>=1.0.1->llama-index-core<0.13.0,>=0.12.0->llama-index-callbacks-arize-phoenix) (0.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-callbacks-arize-phoenix) (2024.11.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3.0.0,>=2.7.0->strawberry-graphql==0.270.1->arize-phoenix) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-callbacks-arize-phoenix) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-callbacks-arize-phoenix) (2.4.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-callbacks-arize-phoenix) (1.1.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography->authlib->arize-phoenix) (1.17.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-callbacks-arize-phoenix) (3.26.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography->authlib->arize-phoenix) (2.22)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api->openinference-instrumentation>=0.1.32->arize-phoenix) (3.23.0)\n",
            "Requirement already satisfied: colorama>=0.4 in /usr/local/lib/python3.11/dist-packages (from griffe->banks<3,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-callbacks-arize-phoenix) (0.4.6)\n",
            "Downloading llama_index_callbacks_arize_phoenix-0.5.1-py3-none-any.whl (3.1 kB)\n",
            "Downloading arize_phoenix-11.2.0-py3-none-any.whl (4.1 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading strawberry_graphql-0.270.1-py3-none-any.whl (301 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m301.2/301.2 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.16.2-py3-none-any.whl (242 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m242.7/242.7 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading arize_phoenix_evals-0.21.0-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m63.2/63.2 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading arize_phoenix_otel-0.12.1-py3-none-any.whl (13 kB)\n",
            "Downloading openinference_instrumentation-0.1.34-py3-none-any.whl (28 kB)\n",
            "Downloading openinference_instrumentation_llama_index-4.3.1-py3-none-any.whl (28 kB)\n",
            "Downloading openinference_semantic_conventions-0.1.21-py3-none-any.whl (10 kB)\n",
            "Downloading sqlean_py-3.49.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aioitertools-0.12.0-py3-none-any.whl (24 kB)\n",
            "Downloading arize_phoenix_client-1.11.0-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading authlib-1.6.0-py2.py3-none-any.whl (239 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m240.0/240.0 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading email_validator-2.2.0-py3-none-any.whl (33 kB)\n",
            "Downloading opentelemetry_exporter_otlp-1.34.1-py3-none-any.whl (7.0 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_http-1.34.1-py3-none-any.whl (17 kB)\n",
            "Downloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_core-3.2.6-py3-none-any.whl (203 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_instrumentation-0.55b1-py3-none-any.whl (31 kB)\n",
            "Installing collected packages: sqlean-py, openinference-semantic-conventions, graphql-core, dnspython, aioitertools, strawberry-graphql, email-validator, alembic, authlib, arize-phoenix-evals, arize-phoenix-client, opentelemetry-instrumentation, opentelemetry-exporter-otlp-proto-http, openinference-instrumentation, opentelemetry-exporter-otlp, openinference-instrumentation-llama-index, llama-index-callbacks-arize-phoenix, arize-phoenix-otel, arize-phoenix\n",
            "Successfully installed aioitertools-0.12.0 alembic-1.16.2 arize-phoenix-11.2.0 arize-phoenix-client-1.11.0 arize-phoenix-evals-0.21.0 arize-phoenix-otel-0.12.1 authlib-1.6.0 dnspython-2.7.0 email-validator-2.2.0 graphql-core-3.2.6 llama-index-callbacks-arize-phoenix-0.5.1 openinference-instrumentation-0.1.34 openinference-instrumentation-llama-index-4.3.1 openinference-semantic-conventions-0.1.21 opentelemetry-exporter-otlp-1.34.1 opentelemetry-exporter-otlp-proto-http-1.34.1 opentelemetry-instrumentation-0.55b1 sqlean-py-3.49.1 strawberry-graphql-0.270.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "llama_index",
                  "opentelemetry"
                ]
              },
              "id": "6e1f6c6357ad4f3a969e24bf83010d79"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install llama-index-callbacks-arize-phoenix arize-phoenix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jSmLawdeTEN"
      },
      "outputs": [],
      "source": [
        "import llama_index\n",
        "import os\n",
        "\n",
        "PHOENIX_API_KEY = \"\"\n",
        "os.environ[\"OTEL_EXPORTER_OTLP_HEADERS\"] = f\"api_key={PHOENIX_API_KEY}\"\n",
        "llama_index.core.set_global_handler(\n",
        "    \"arize_phoenix\", endpoint=\"https://llamatrace.com/v1/traces\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.12 Run the agent and analyze responses"
      ],
      "metadata": {
        "id": "434C1PNgu2YT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxC0y5llE3dB",
        "outputId": "f5a5ba5b-3d1b-430c-ff5b-20f2ba0844c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;3;38;5;200mSelecting query engine 1: The question 'Yareina Lee' is more likely to be related to retrieving specific context from the State of AI paper rather than summarization questions..\n",
            "\u001b[0mLareina Yee is a director of the McKinsey Global Institute and a senior partner in the Bay Area office, as mentioned in the document.\n"
          ]
        }
      ],
      "source": [
        "# In Jupyter/Colab, you can use await directly\n",
        "question = \"Who is Yareina Lee according to the document? Where is she mentioned in the document and in what context?\"\n",
        "response = await query_engine_agent.run(question)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.13 Equip the agent with multiple tools"
      ],
      "metadata": {
        "id": "1PxekjRYu5HV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index-tools-arxiv llama-index-tools-wikipedia duckduckgo-search\n",
        "!pip install llama-index-tools-brave-search\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1fkBji-joOLP",
        "outputId": "50fac1d7-7f1b-4f18-e627-a5621fba50b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-index-tools-arxiv\n",
            "  Downloading llama_index_tools_arxiv-0.3.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting llama-index-tools-wikipedia\n",
            "  Downloading llama_index_tools_wikipedia-0.3.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting duckduckgo-search\n",
            "  Downloading duckduckgo_search-8.0.4-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting arxiv<3.0.0,>=2.1.0 (from llama-index-tools-arxiv)\n",
            "  Downloading arxiv-2.2.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-tools-arxiv) (0.12.45)\n",
            "Collecting wikipedia<2.0,>=1.4 (from llama-index-tools-wikipedia)\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.11/dist-packages (from duckduckgo-search) (8.2.1)\n",
            "Collecting primp>=0.15.0 (from duckduckgo-search)\n",
            "  Downloading primp-0.15.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: lxml>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from duckduckgo-search) (5.4.0)\n",
            "Collecting feedparser~=6.0.10 (from arxiv<3.0.0,>=2.1.0->llama-index-tools-arxiv)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: requests~=2.32.0 in /usr/local/lib/python3.11/dist-packages (from arxiv<3.0.0,>=2.1.0->llama-index-tools-arxiv) (2.32.3)\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (3.11.15)\n",
            "Requirement already satisfied: aiosqlite in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (0.21.0)\n",
            "Requirement already satisfied: banks<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (2.1.3)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2,>=1.0.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (1.0.8)\n",
            "Requirement already satisfied: filetype<2,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (2025.3.2)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (0.28.1)\n",
            "Requirement already satisfied: llama-index-workflows<2,>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (1.0.1)\n",
            "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (3.5)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (2.0.2)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (11.2.1)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (2.11.7)\n",
            "Requirement already satisfied: pyyaml>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (6.0.2)\n",
            "Requirement already satisfied: setuptools>=80.9.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (80.9.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (2.0.41)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (8.5.0)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (0.9.0)\n",
            "Requirement already satisfied: tqdm<5,>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (4.14.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (1.17.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from wikipedia<2.0,>=1.4->llama-index-tools-wikipedia) (4.13.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (1.20.1)\n",
            "Requirement already satisfied: griffe in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (1.7.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (3.1.6)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (4.3.8)\n",
            "Collecting sgmllib3k (from feedparser~=6.0.10->arxiv<3.0.0,>=2.1.0->llama-index-tools-arxiv)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: llama-index-instrumentation>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-workflows<2,>=1.0.1->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (0.2.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (2024.11.6)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.0->arxiv<3.0.0,>=2.1.0->llama-index-tools-arxiv) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.0->arxiv<3.0.0,>=2.1.0->llama-index-tools-arxiv) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.0->arxiv<3.0.0,>=2.1.0->llama-index-tools-arxiv) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.0->arxiv<3.0.0,>=2.1.0->llama-index-tools-arxiv) (2025.6.15)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (3.2.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (1.1.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->wikipedia<2.0,>=1.4->llama-index-tools-wikipedia) (2.7)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (3.26.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (0.16.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (24.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (1.3.1)\n",
            "Requirement already satisfied: colorama>=0.4 in /usr/local/lib/python3.11/dist-packages (from griffe->banks<3,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->banks<3,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (3.0.2)\n",
            "Downloading llama_index_tools_arxiv-0.3.0-py3-none-any.whl (2.5 kB)\n",
            "Downloading llama_index_tools_wikipedia-0.3.0-py3-none-any.whl (2.7 kB)\n",
            "Downloading duckduckgo_search-8.0.4-py3-none-any.whl (18 kB)\n",
            "Downloading arxiv-2.2.0-py3-none-any.whl (11 kB)\n",
            "Downloading primp-0.15.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: wikipedia, sgmllib3k\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11757 sha256=2f1ddadc8d2536c101f72e20ba288fc4801d6e42c1dc39439d79c7a2004f0ffb\n",
            "  Stored in directory: /root/.cache/pip/wheels/8f/ab/cb/45ccc40522d3a1c41e1d2ad53b8f33a62f394011ec38cd71c6\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6089 sha256=2bf5ee7c20de81295d624fad8cf50439ce5987f621922e7315a1ea10806886da\n",
            "  Stored in directory: /root/.cache/pip/wheels/3b/25/2a/105d6a15df6914f4d15047691c6c28f9052cc1173e40285d03\n",
            "Successfully built wikipedia sgmllib3k\n",
            "Installing collected packages: sgmllib3k, primp, feedparser, wikipedia, duckduckgo-search, arxiv, llama-index-tools-wikipedia, llama-index-tools-arxiv\n",
            "Successfully installed arxiv-2.2.0 duckduckgo-search-8.0.4 feedparser-6.0.11 llama-index-tools-arxiv-0.3.0 llama-index-tools-wikipedia-0.3.0 primp-0.15.0 sgmllib3k-1.0.0 wikipedia-1.4.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "llama_index"
                ]
              },
              "id": "ac88349329834b8091246e616464ae2b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-index-tools-brave-search\n",
            "  Downloading llama_index_tools_brave_search-0.3.0-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-tools-brave-search) (0.12.45)\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (3.11.15)\n",
            "Requirement already satisfied: aiosqlite in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (0.21.0)\n",
            "Requirement already satisfied: banks<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (2.1.3)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2,>=1.0.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (1.0.8)\n",
            "Requirement already satisfied: filetype<2,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (2025.3.2)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (0.28.1)\n",
            "Requirement already satisfied: llama-index-workflows<2,>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (1.0.1)\n",
            "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (3.5)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (2.0.2)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (11.2.1)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (2.11.7)\n",
            "Requirement already satisfied: pyyaml>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=80.9.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (80.9.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (2.0.41)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (8.5.0)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (0.9.0)\n",
            "Requirement already satisfied: tqdm<5,>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (4.14.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (1.17.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (1.20.1)\n",
            "Requirement already satisfied: griffe in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (1.7.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (3.1.6)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (4.3.8)\n",
            "Requirement already satisfied: llama-index-instrumentation>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-workflows<2,>=1.0.1->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (0.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (2024.11.6)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (2025.6.15)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (3.2.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (1.1.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (3.26.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (0.16.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (24.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (1.3.1)\n",
            "Requirement already satisfied: colorama>=0.4 in /usr/local/lib/python3.11/dist-packages (from griffe->banks<3,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->banks<3,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (3.0.2)\n",
            "Downloading llama_index_tools_brave_search-0.3.0-py3-none-any.whl (2.8 kB)\n",
            "Installing collected packages: llama-index-tools-brave-search\n",
            "Successfully installed llama-index-tools-brave-search-0.3.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "llama_index"
                ]
              },
              "id": "6bb27bc1ee144586bb13f3d430f27c09"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.14 Add the new tools (ArXiV, Brave Search)\n"
      ],
      "metadata": {
        "id": "16E17tfjvLIx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import additional tools\n",
        "from llama_index.tools.arxiv import ArxivToolSpec\n",
        "from llama_index.tools.wikipedia import WikipediaToolSpec\n",
        "from llama_index.core.tools import QueryEngineTool\n",
        "from llama_index.tools.brave_search import BraveSearchToolSpec\n",
        "\n",
        "import requests\n",
        "import json\n"
      ],
      "metadata": {
        "id": "soxyYOVfoAHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create ArXiV tool\n",
        "\n",
        "arxiv_tool = ArxivToolSpec()\n",
        "\n",
        "arxiv_tools = arxiv_tool.to_tool_list()\n",
        "\n",
        "\n",
        "# Create Brave Search tool\n",
        "\n",
        "brave_search_tool_spec = BraveSearchToolSpec(api_key=\"\")\n",
        "brave_search_tools = brave_search_tool_spec.to_tool_list()\n"
      ],
      "metadata": {
        "id": "m2lC5H1SoYpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create enhanced agent with multiple tools - FIX: Use extend instead of append\n",
        "enhanced_tools = [query_engine_tool]  # Start with McKinsey report tool\n",
        "enhanced_tools.extend(brave_search_tools)  # Add all brave search tools\n",
        "enhanced_tools.extend(arxiv_tools)  # Add all arxiv tools"
      ],
      "metadata": {
        "id": "Rp0WSiqpodii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.15 Define the enhanced agent with all tools\n"
      ],
      "metadata": {
        "id": "RjWiEqbHvSmE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create new enhanced agent\n",
        "enhanced_agent = AgentWorkflow.from_tools_or_functions(\n",
        "    tools_or_functions=enhanced_tools,\n",
        "    llm=Settings.llm,\n",
        "    system_prompt=\"\"\"You are an AI research assistant with access to:\n",
        "    1. The McKinsey 2025 State of AI report\n",
        "    2. Web search capabilities\n",
        "    3. ArXiv research paper search\n",
        "\n",
        "    Use these tools to provide comprehensive, well-researched answers. When discussing AI trends,\n",
        "    combine insights from the McKinsey report with recent research and web findings.\"\"\",\n",
        ")\n"
      ],
      "metadata": {
        "id": "QFaTmUITooBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.16 Battle test agent with multiple questions!\n"
      ],
      "metadata": {
        "id": "31wclWR0vWVJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test questions that can benefit from multiple tools\n",
        "\n",
        "# Question 1: Combine McKinsey insights with recent research\n",
        "question1 = \"\"\"According to the McKinsey report, what are the main organizational changes companies are making for AI adoption?\n",
        "Can you also search for recent research papers on AI governance and organizational transformation to provide additional context?\"\"\"\n",
        "\n",
        "print(\"Question 1: Organizational changes and governance\")\n",
        "print(\"=\" * 50)\n",
        "response1 = await enhanced_agent.run(question1)\n",
        "print(response1)\n",
        "print(\"\\n\" + \"=\"*80 + \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvuCQ2GAoqmQ",
        "outputId": "3e0fb64c-3efe-4d8e-ed83-7d338f230f77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question 1: Organizational changes and governance\n",
            "==================================================\n",
            "\u001b[1;3;38;5;200mSelecting query engine 0: Summarization questions related to the State of AI paper would likely cover the main organizational changes companies are making for AI adoption..\n",
            "\u001b[0m### Recent Research Papers on AI Governance and Organizational Transformation:\n",
            "\n",
            "#### AI Governance Research Papers:\n",
            "1. **[AI And Organizational Change: Dynamics And Management Strategies](https://www.researchgate.net/publication/380929689_AI_And_Organizational_Change_Dynamics_And_Management_Strategies)**\n",
            "   - This study investigates the dynamics of AI-induced organizational change, focusing on effective change management strategies, employee adaptation, and cultural transformation.\n",
            "\n",
            "2. **[AI in Organizational Change Management \u2014 Case Studies, Best Practices, Ethical Implications, and Future Technological Trajectories](https://medium.com/@adnanmasood/ai-in-organizational-change-management-case-studies-best-practices-ethical-implications-and-179be4ec2583)**\n",
            "   - Details the technical and methodological aspects of integrating AI into organizational change, including case studies, best practices, ethical considerations, and future trajectories.\n",
            "\n",
            "3. **[Artificial Intelligence and Organizational Development: Psychological Perspectives on the Restructuring of Organizational Structures, Processes, and Functions](https://www.researchgate.net/publication/381094544_Artificial_Intelligence_and_Organizational_Development_Psychological_Perspectives_on_the_Restructuring_of_Organizational_Structures_Processes_and_Functions)**\n",
            "   - Explores the psychological perspectives on restructuring organizational structures, processes, and functions with the integration of artificial intelligence.\n",
            "\n",
            "#### Organizational Transformation AI Research Papers:\n",
            "1. **[GENERATIVE AI IN CHANGE MANAGEMENT: A NEW FRAMEWORK FOR ORGANIZATIONAL TRANSFORMATION](https://www.researchgate.com/publication/379765500_GENERATIVE_AI_IN_CHANGE_MANAGEMENT_A_NEW_FRAMEWORK_FOR_ORGANIZATIONAL_TRANSFORMATION)**\n",
            "   - Discusses a new framework for organizational transformation using generative AI in change management, emphasizing the importance of effective change management for organizations to adapt and thrive in evolving landscapes.\n",
            "\n",
            "2. **[Navigating the Organizational AI Journey: The AI Transformation Framework](https://www.sciencedirect.com/science/article/abs/pii/S0007681325000023)**\n",
            "   - Presents the AI transformation framework, offering recommendations for practice and highlighting the broad spectrum of changes and adaptations that organizations undergo to leverage the full potential of AI technologies.\n",
            "\n",
            "These research papers provide valuable insights into AI governance, organizational change dynamics, and the integration of AI in driving organizational transformations.\n",
            "\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 2: Workflow Redesign and Implementation\n",
        "question2 = \"\"\"What does the McKinsey report say about workflow redesign for AI implementation?\n",
        "Search ArXiv for papers on business process automation with AI and find current web articles about workflow transformation.\"\"\"\n",
        "\n",
        "print(\"Question 2: Workflow Redesign\")\n",
        "response2 = await enhanced_agent.run(question2)\n",
        "print(response2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-99uMZimpCXj",
        "outputId": "c368957f-5dc5-43ab-f1a7-6ccb8abddd86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question 2: Workflow Redesign\n",
            "\u001b[1;3;38;5;200mSelecting query engine 0: Workflow redesign for AI implementation may involve summarizing key points and recommendations from the State of AI paper..\n",
            "\u001b[0m### ArXiv Papers on Business Process Automation with AI:\n",
            "1. **[D3BA: A Tool for Optimizing Business Processes Using Non-Deterministic Planning](http://arxiv.org/pdf/2001.02619v2):**\n",
            "   - This paper introduces D3BA, a tool for optimizing business processes using AI planning. It focuses on composing services to automate subtasks within complex business processes.\n",
            "\n",
            "2. **[Can Artificial Intelligence Transform DevOps?](http://arxiv.org/pdf/2206.00225v1):**\n",
            "   - Explores the connection between DevOps and AI, highlighting how AI can enhance DevOps processes such as testing, coding, releasing, monitoring, and system improvement.\n",
            "\n",
            "3. **[Impact of Artificial Intelligence on Businesses](http://arxiv.org/pdf/1905.02092v1):**\n",
            "   - Discusses the integration of AI in business processes and its impact on research, innovation, market deployment, and future shifts in business models.\n",
            "\n",
            "### Web Articles on Workflow Transformation:\n",
            "1. **[How Workflow Automation Delivers Business Transformation](https://www.flowforma.com/blog/how-workflow-automation-delivers-business-transformation):**\n",
            "   - Explores workflow automation and its role in delivering business transformation by making processes more intuitive, efficient, and effective.\n",
            "\n",
            "2. **[Workflow Transformation | Buddha Logic](https://buddhalogic.com/solutions/workflow-transformation/):**\n",
            "   - Describes workflow transformation as the process of examining content-focused processes and designing advanced software solutions to enhance efficiency.\n",
            "\n",
            "3. **[Digital Workflow Transformation | Business Process Automation | Kofax](https://www.tungstenautomation.com/workflows):**\n",
            "   - Focuses on automating processes to manage and scale for the future, emphasizing end-to-end digital business workflow transformation.\n",
            "\n",
            "4. **[Workflow > Data Transformation](https://support.medit.com/hc/en-us/articles/360052595291--Workflow-Data-Transformation):**\n",
            "   - Discusses data transformation within workflows, allowing users to adjust data size and scale for various applications.\n",
            "\n",
            "5. **[Why Digital Transformation Starts with Workflows | IBM](https://www.ibm.com/think/insights/why-digital-transformation-starts-with-workflows-that-unite-people-and-technology):**\n",
            "   - Highlights the importance of workflows that unite people and technology in digital transformation, enabling seamless global trade and rapid deployment.\n",
            "\n",
            "These resources provide insights into AI-driven business process optimization and the significance of workflow transformation in achieving business objectives.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 3: Risk management and future trends\n",
        "question3 = \"\"\"Based on the McKinsey report, what are the key risks organizations are addressing with gen AI?\n",
        "Can you search the web for recent academic research on AI risk mitigation and compare with the report's findings?\"\"\"\n",
        "\n",
        "print(\"Question 3: Risk management\")\n",
        "response3 = await enhanced_agent.run(question3)\n",
        "print(response3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ieLf-4v8qB8C",
        "outputId": "82c79861-d0f4-4b97-9e5a-c80058d5e5b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question 3: Risk management\n",
            "\u001b[1;3;38;5;200mSelecting query engine 0: The choice 'Useful for summarization questions related to the State of AI paper' is most relevant as it focuses on summarizing key risks organizations are addressing with gen AI, which aligns with the question asked..\n",
            "\u001b[0m### Recent Academic Research on AI Risk Mitigation:\n",
            "1. **FADRM: Fast and Accurate Data Residual Matching for Dataset Distillation**:\n",
            "   - This research introduces the concept of Data Residual Matching to facilitate data generation and mitigate data information vanishing in dataset distillation tasks. The method significantly improves computational efficiency and achieves superior performance across multiple dataset benchmarks.\n",
            "   - [Read more](http://arxiv.org/pdf/2506.24125v1)\n",
            "\n",
            "2. **Scaling Human Judgment in Community Notes with LLMs**:\n",
            "   - The paper proposes an open ecosystem where both humans and LLMs can write notes, with human raters serving as the ultimate evaluator of helpful notes. This approach accelerates note delivery while maintaining trust and legitimacy.\n",
            "   - [Read more](http://arxiv.org/pdf/2506.24118v1)\n",
            "\n",
            "3. **Development of Hybrid Artificial Intelligence Training on Real and Synthetic Data**:\n",
            "   - Synthetic data is explored as a cost-effective alternative for training artificial neural networks. The study analyzes mixing strategies of synthetic and real data to bridge the domain gap and enhance the robustness and efficacy of training processes.\n",
            "   - [Read more](http://arxiv.org/pdf/2506.24093v1)\n",
            "\n",
            "### Comparison with McKinsey Report Findings:\n",
            "The academic research aligns with the McKinsey report's emphasis on mitigating risks associated with AI. Both sources highlight the importance of addressing data-related risks, improving computational efficiency, and enhancing performance through innovative approaches like data residual matching and hybrid training strategies. Organizations can benefit from integrating these research insights into their risk mitigation strategies to navigate the challenges posed by AI technologies effectively.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question that forces usage of all three tools\n",
        "comprehensive_question = \"\"\"Who is Lareina Yee in the McKinsey document and what are her views on AI's workforce impact?\n",
        "\n",
        "After finding information about her from the document, please:\n",
        "1. Search the web using Brave Search for recent articles, interviews, or news about Lareina Yee and her work on AI\n",
        "2. Search ArXiv for any research papers she may have authored or co-authored related to AI, workforce transformation, or economic impact\n",
        "3. Provide a comprehensive profile combining insights from all three sources about her expertise and contributions to AI research\"\"\"\n",
        "\n",
        "print(\"Question: Comprehensive profile of Lareina Yee\")\n",
        "print(\"=\" * 60)\n",
        "print(\"This question should force the agent to use:\")\n",
        "print(\"1. Query Engine - to find info about Lareina Yee in the McKinsey document\")\n",
        "print(\"2. Brave Search - to find recent web articles/news about her\")\n",
        "print(\"3. ArXiv Search - to find any academic papers she's authored\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "response = await enhanced_agent.run(comprehensive_question)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUIlpdi8qB5_",
        "outputId": "c0f08538-4edc-4af9-8553-4cd53fb315d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: Comprehensive profile of Lareina Yee\n",
            "============================================================\n",
            "This question should force the agent to use:\n",
            "1. Query Engine - to find info about Lareina Yee in the McKinsey document\n",
            "2. Brave Search - to find recent web articles/news about her\n",
            "3. ArXiv Search - to find any academic papers she's authored\n",
            "============================================================\n",
            "\u001b[1;3;38;5;200mSelecting query engine 1: The question is asking for specific context related to 'Lareina Yee', which would require retrieving specific information from the State of AI paper..\n",
            "\u001b[0m### Lareina Yee Profile:\n",
            "\n",
            "#### McKinsey Global Institute Director:\n",
            "Lareina Yee is a Senior Partner and the Director of the McKinsey Global Institute, where she leads research on AI and frontier technologies, advising companies on growth and transformation.\n",
            "\n",
            "#### Views on AI's Workforce Impact:\n",
            "Lareina Yee's work focuses on understanding the impact of AI on the workforce, particularly in terms of transformation and economic implications.\n",
            "\n",
            "#### Recent Web Findings:\n",
            "1. **[McKinsey Article: Superagency in the Workplace](https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/superagency-in-the-workplace-empowering-people-to-unlock-ais-full-potential-at-work)**: Discusses how AI is being used in the workplace in 2025.\n",
            "2. **[McKinsey Article: Preparing for Tomorrow's Agentic Workforce](https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/preparing-for-tomorrows-agentic-workforce)**: Mentions Lareina Yee's insights on AI inferencing and its importance.\n",
            "3. **[LinkedIn Profile](https://www.linkedin.com/in/lareinayee/)**: Highlights Lareina Yee's vision for responsible AI research and development.\n",
            "\n",
            "#### Research Papers on ArXiv:\n",
            "1. **[Refinement of Partition Identities by Merca and Yee](http://arxiv.org/pdf/2111.10587v1)**: Refinement and generalization of partition identities by Merca and Yee.\n",
            "2. **[Identities Associated with Mock Theta Functions by Andrews, Dixit, and Yee](http://arxiv.org/pdf/1709.03213v1)**: Defines partition functions related to Ramanujan's mock theta functions.\n",
            "3. **[Combinatorial Proof of an Identity by Andrews and Yee](http://arxiv.org/pdf/1710.10373v2)**: Presents a combinatorial proof of an identity studied by Andrews and Yee.\n",
            "\n",
            "Lareina Yee's expertise lies in AI research, workforce transformation, and economic impact analysis, making significant contributions to understanding the implications of AI on businesses and society.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}